### Было рассмотрено 4 варианта: CPU, CPU + JIT, CUDA + JIT, numpy.dot
#### " JIT-компиляция позволяет интерпретируемому языку, такому как Python, динамически компилировать части кода в машинный код, значительно ускоряя исполнение. Numba использует этот подход, чтобы помочь вашему коду на Python быть быстрее. Она анализирует вашу функцию, определяет типы данных и затем компилирует её в оптимизированный машинный код. Всё это происходит во время выполнения вашего кода. Когда вы помечаете функцию декоратором @jit, Numba анализирует вашу функцию и определяет, как её можно оптимизировать. Она смотрит на типы переменных и операции, которые вы выполняете, и на основе этого создаёт оптимизированный машинный код."
#### Для замера времени функции запускались по 12 раз
#### Особенностью JIT является то, что при первом запуске функция отрабатывает гораздо большее время, чем при последующих
#### Умножение матриц - идеальная задача для распараллеливания на GPU, так как каждый элемент итоговой матрицы не зависит от других элементов и поэтому может быть рассчитан отдельным потоком
#### Все 3 варианта показали ускорение относительно наивной реализации, но реализация на CUDA показала наибольшее ускорение в размере почти 7000!
#### CPU JIT и numpy.dot показали почти идентичные результаты
#### Добавлено сравнение с последовательным вариантом на C
#### На максимальной размерности GPU показало ускорение относительно C в ~40 раз
#### Можно сделать вывод, что огромное ускорение обусловлено не только использованием GPU, но и тем фактом, что Python достаточно медленный язык и приведенная последовательная реализация на Python написана максимально не оптимально